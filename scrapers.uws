function get_scrapers()
    items = [
        Scraper(
            "児玉まりあ文学集成",
            'https://to-ti.in/product/mariakodama',
            ScrapeToti,
            WeekDay.Sun, AM
        ),
        Scraper(
            "無能なナナ",
            'https://www.ganganonline.com/title/72',
            ScrapeGanGanOnline,
            WeekDay.Sat, PM
        ),
        Scraper(
            "ブレイド＆バスタード",
            'https://drecom-media.jp/drecomics/series/blade',
            ScrapeDrecom,
            WeekDay.Fri, PM
        ),
        Scraper(
            "俺、勇者じゃないですから。",
            'https://bunshun.jp/list/author/60e7c5b077656106fe030000',
            ScrapeBunshun,
            WeekDay.Fri, PM
        ),
        Scraper(
            "神引きのモナーク（仮）",
            'https://www.pixiv.net/user/1504297/series/198227',
            ScrapePixiv,
            WeekDay.Every3Days, AM
        ),
        // Scraper(
        //     "ニセモノの錬金術師",
        //     'https://www.pixiv.net/user/1504297/series/74316',
        //     ScrapePixiv,
        //     WeekDay.Tue, 4
        // ),
        // Scraper(
        //     "ニセモノの錬金術師：第二部",
        //     'https://www.pixiv.net/user/1504297/series/135359',
        //     ScrapePixiv,
        //     WeekDay.Wed, 4
        // ),
        // Scraper(
        //     "ニセモノの錬金術師：第百部",
        //     'https://www.pixiv.net/user/1504297/series/141994',
        //     ScrapePixiv,
        //     WeekDay.Thu, 4
        // ),
        Scraper(
            "決闘手術",
            'https://www.pixiv.net/user/63740794/series/132782',
            ScrapePixiv,
            WeekDay.Sun, AM
        ),
        // Scraper(
        //     "dummy", "",
        //     |key, url, check=>get_feed_object('subtitle', 'link', 'summary', 0.0)|,
        //     WeekDay.Any, AM
        // )
    ]
    result = Scrapers(items)
fend

class Scrapers
    dim items
    procedure Scrapers(items: array)
        this.items = items
    fend
    function get(i: number)
        result = this.item[i]
    fend
    function get_by_title(title: string)
        result = EMPTY
        for item in items
            if pos(title, item.title()) > 0 then
                result = item
                break
            endif
        next
    fend
    function as_array()
        result = this.items
    fend
endclass

class PubDate
    dim dt_sec
    // constructor
    procedure PubDate(dt: number = 0)
        if dt > 0 then
            this.dt_sec = dt
        else
            this.dt_sec = gettime()
        endif
    fend

    function get_sec()
        result = this.dt_sec
    fend

    function to_string()
        result = format(this.dt_sec, '%a, %d %h %Y %T %z', FALSE, 'en_US')
    fend

endclass

function get_feed_object(title, link, description, pub_dt_sec: number = 0)
    result = @{
        "title": null,
        "link": null,
        "pubDate": null,
        "pubSec": null,
        "description": null
    }@
    result.title = title
    result.link = link
    result.description = description
    pd = PubDate(pub_dt_sec)
    result.pubDate = pd.to_string()
    result.pubSec = pd.get_sec()
fend

function convert_date_to_sec(date, offset = 0, offset_opt = G_OFFSET_HOURS)
    select TRUE
        // 年月日 表記
        // コミックガンマ
        // DRECOM
        case length(m := Match(date, '(\d{4})年(\d{1,2})月(\d{1,2})日')) > 0
            yyyy = m[0][1]
            mm = format(val(m[0][2]), 2, 0, FMT_ZERO)
            dd = format(val(m[0][3]), 2, 0, FMT_ZERO)
            result = gettime(offset, "<#yyyy>/<#mm>/<#dd>", offset_opt)
        // yy/mm/dd 表記
        // トーチ
        case length(m := Match(date, "\d{2}/\d{2}/\d{2}")) > 0
            d = '20' + m[0]
            result = gettime(offset, d, offset_opt)
        // yyyy.mm.dd 表記
        // ガンガンONLINE
        case length(m := Match(date, "\d{4}\.\d{2}\.\d{2}")) > 0
            d = replace(m[0], '.', '/')
            result = gettime(offset, d, offset_opt)
        // yyyy/mm/dd 表記
        // カドコミ
        case length(m := Match(date, "\d{4}/\d{2}/\d{2}")) > 0
            d = m[0]
            result = gettime(offset, d, offset_opt)
        // yyyy-mm-dd hh:MM 表記
        // 文春オンライン
        case length(m := Match(date, "\d{4}-\d{2}-\d{2} \d{2}:\d{2}")) > 0
            d = m[0] + ':00'
            result = gettime(offset, d, offset_opt)
        default
            result = 0
    selend
fend


function ScrapePixiv(key: string, url: string, ignore_update_check: bool = FALSE, page = 1)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        result = []
        links = tab.document.querySelectorAll('main ul li a:nth-last-child(1)')
        first_link = EMPTY
        for link, i in links
            if FeedScraperHelper.check(key, link.href) then
                if i == 0 and page == 1 then
                    // 1ページ目1番目が更新されていたらiniを更新
                    first_link = link.href
                endif
                subtitle = trim( link.textContent )
                subtitle = "<#subtitle> (<#key>)"
                summary = trim( subtitle )
                pub_sec = 0
                result += get_feed_object(subtitle, link.href, summary, pub_sec)
            else
                break
            endif
        next
        // リンクの数とフィードの数が一致していた場合2ページ目を考慮する
        if length(result) == length(links) and page < 2 then
            page += 1
            for a in tab.document.querySelectorAll('main nav a')
                if trim( a.textContent ) == "<#page>" then
                    a.click()
                    tab.wait()
                    result = ScrapePixiv(key, a.href, ignore_update_check, page)
                    break
                endif
            next
        endif
        if first_link != EMPTY then
            FeedScraperHelper.update(key, first_link)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeBunshun(key: string, url: string, ignore_update_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        li = tab.document.querySelector('#page-content ul li')
        link = li.querySelector('.title a')
        if ignore_update_check or FeedScraperHelper.check(key, link.href) then
            subtitle = trim( li.querySelector('.subtitle').textContent )
            summary = trim( link.textContent )
            pub_sec = convert_date_to_sec(li.querySelector('time').dateTime, 0)
            result = get_feed_object(subtitle, link.href, summary, pub_sec)
            FeedScraperHelper.update(key, link.href)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeKadoComi(key: string, url: string, ignore_update_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        root = tab.document.querySelector('.EpisodesTabContents_root__VrJZ1')
        link = root.querySelector('ul li a')
        if ignore_update_check or FeedScraperHelper.check(key, link.href) then
            subtitle = trim( link.querySelector('.EpisodeThumbnail_title__G1eWj').textContent )
            subtitle = "<#subtitle> (<#key>)"
            summary = trim( root.querySelector('.EpisodesTabContents_nextUpdateDate__YDQiC').textContent )
            pub_sec = convert_date_to_sec(trim( link.querySelector('span').textContent ), 0)
            result = get_feed_object(subtitle, link.href, summary, pub_sec)
            FeedScraperHelper.update(key, link.href)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeDrecom(key: string, url: string, ignore_update_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        div = tab.document.querySelector('.ebookListItem')
        link = div.querySelector('a').href
        if ignore_update_check or FeedScraperHelper.check(key, link) then
            subtitle = trim(div.querySelector('.ebookListItem_title').textContent)
            subtitle = "<#subtitle> (<#key>)"
            spans = div.querySelectorAll('.ebookListItem_publishDate span')
            summary = trim( spans[1].textContent )
            pub_sec = convert_date_to_sec(trim( spans[0].textContent ), 12)
            result = get_feed_object(subtitle, link, summary, pub_sec)
            FeedScraperHelper.update(key, link)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeGanGanOnline(key: string, url: string, ignore_update_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        link = tab.document.querySelectorAll('.Chapter_chapter__oSTBF')[1]
        inner = link.querySelectorAll('.Chapter_chapter__body__Iopda p')
        episode = trim( inner[0].textContent )
        subtitle = "<#episode> (<#key>)"
        summary = trim( inner[1].textContent )
        if ignore_update_check or FeedScraperHelper.check(key, episode) then
            pub_sec = convert_date_to_sec(trim( inner[2].textContent ), 8)
            // urlを得るために実際にリンクをクリック
            link.click()
            // 読み込み待ち
            tab.wait()
            // URL取得
            sleep(1)
            href = tab.document.location.href
            result = get_feed_object(subtitle, href, summary, pub_sec)
            FeedScraperHelper.update(key, episode)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeComicGamma(key: string, url: string, ignore_update_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        link = tab.document.querySelector('div.read__outer a')
        if ignore_update_check or FeedScraperHelper.check(key, link.id) then
            subtitle = trim( link.querySelector('.episode').textContent )
            subtitle = "<#subtitle> (<#key>)"
            updated = link.querySelector('.episode__text').textContent
            pub_sec = convert_date_to_sec(updated, 12)
            summary = '公開日: ' + updated
            result = get_feed_object(subtitle, link.href, summary, pub_sec)
            FeedScraperHelper.update(key, link.id)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeToti(key: string, url: string, ignore_update_check: bool = FALSE)
    result = EMPTY

    tab = FeedScraperHelper.browser.new(url)
    try
        link = tab.document.querySelector('.next a')
        if ignore_update_check or FeedScraperHelper.check(key, link.href) then
            subtitle = trim( link.querySelector('span.typesquare_option').textContent )
            subtitle = "<#subtitle> (<#key>)"
            summary = trim( tab.document.querySelector('header time').textContent )
            pub_sec = convert_date_to_sec(summary)
            result = get_feed_object(subtitle, link.href, summary, pub_sec)
            FeedScraperHelper.update(key, link.href)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

module FeedScraperHelper
    const HEADLESS = FALSE
    const MANGA_FEED_PROFILE_DIR = "<#GET_CUR_DIR>\profile\"
    // const ERROR_LOG = 'D:\manga_feed\error.log'
    public browser
    // constructor
    procedure FeedScraperHelper
        browser = browserbuilder(BC_MSEDGE)_
            .headless(HEADLESS)_
            .profile(MANGA_FEED_PROFILE_DIR)_
            .start()
    fend

    const ini = 'manga_feed.ini'
    const section = 'manga_feed'

    function check(key, value)
        recent = readini(section, key, ini)
        result = value != recent
    fend

    procedure update(key, value)
        writeini(section, key, value, ini)
    fend

    const section_next_dt = 'next'

    function get_next_dt(key)
        next_check = readini(section_next_dt, key, ini)
        result = val(next_check, 0)
    fend

    procedure set_next_dt(key, value)
        writeini(section_next_dt, key, value, ini)
    fend


    // procedure error(msg)
    //     fid = fopen(ERROR_LOG, F_WRITE8 or F_AUTOCLOSE)
    //     fput(fid, msg)
    // fend
endmodule


class Scraper
    // タイトル
    dim title
    // URL
    dim url
    // 曜日
    dim day
    // 時刻
    dim time
    // スクレイピング用関数
    dim fn
    // constructor
    procedure Scraper(title: string, url: string, fn: func, day: WeekDay, time: number)
        this.title = title
        this.url = url
        this.fn = fn
        this.day = day
        this.time = time
    fend

    // 次回確認日時以降かどうかを確認
    function check()
        last = this.get_check_dt()
        now = gettime()
        result = last < now
    fend

    // スクレイピング用関数はUObject、またはEMPTYを返す
    function scrape(ignore_udpate_check = FALSE)
        result = fn(this.title, this.url, ignore_udpate_check)
        this.update_next_check_dt()
    fend

    function title()
        result = this.title
    fend
    // 次回チェック日時を得る
    function get_check_dt()
        result = FeedScraperHelper.get_next_dt(this.title)
    fend
    // フォーマットされた次回チェック日時を得る
    function get_check_fmt()
        n = this.get_check_dt()
        result = format(n, '%x(%a)の%p')
    fend
    // 次回チェック日時を得る
    procedure update_next_check_dt()
        dim format_to_date_string = '%F'
        last = this.get_check_dt()
        if last > 0 then
            // 前回から七日進める
            date = format(last, format_to_date_string)
            shift = this.get_shift_date()
            next_check = gettime(shift, date, G_OFFSET_DAYS)
            FeedScraperHelper.set_next_dt(this.title, next_check)
        else
            // 次の該当曜日を得る
            today = format(gettime(), format_to_date_string)
            dim days
            select this.day
                case WeekDay.Any, WeekDay.EveryDay
                    days = 1
                case WeekDay.Every2Days
                    days = 2
                case WeekDay.Every3Days
                    days = 3
                case WeekDay.Every4Days
                    days = 4
                case WeekDay.Every5Days
                    days = 5
                case WeekDay.Every6Days
                    days = 6
                case WeekDay.Every7Days
                    days = 7
                default
                    gtww = weekday_to_g_time_ww[this.day]
                    days = gtww - G_TIME_WW
                    if days < 0 then
                        days += 7
                    endif
            selend
            next_check = gettime(days, today, G_OFFSET_DAYS)
            if this.time == PM then
                // 12時間進める
                next_check += 60 * 60 * 12
            endif
            FeedScraperHelper.set_next_dt(this.title, next_check)
        endif
    fend
    function get_shift_date()
        select this.day
            case WeekDay.Any, WeekDay.EveryDay
                result = 1
            case WeekDay.Every2Days
                result = 2
            case WeekDay.Every3Days
                result = 3
            case WeekDay.Every4Days
                result = 4
            case WeekDay.Every5Days
                result = 5
            case WeekDay.Every6Days
                result = 6
            case WeekDay.Every7Days
                result = 7
            default
                result = 7
        selend
    fend

endclass

enum WeekDay
    Mon = 1
    Tue = 2
    Wed = 4
    Thu = 8
    Fri = 16
    Sat = 32
    Sun = 64
    Any = 127
    EveryDay   = $100 // 毎日
    Every2Days = $200 // 2日毎
    Every3Days = $300 // 3日毎
    Every4Days = $400 // 4日毎
    Every5Days = $500 // 5日毎
    Every6Days = $600 // 6日毎
    Every7Days = $700 // 7日毎
endenum
hash public convert_weekday
    "<#G_WEEKDAY_MON>" = WeekDay.Mon
    "<#G_WEEKDAY_TUE>" = WeekDay.Tue
    "<#G_WEEKDAY_WED>" = WeekDay.Wed
    "<#G_WEEKDAY_THU>" = WeekDay.Thu
    "<#G_WEEKDAY_FRI>" = WeekDay.Fri
    "<#G_WEEKDAY_SAT>" = WeekDay.Sat
    "<#G_WEEKDAY_SUN>" = WeekDay.Sun
endhash
hash public weekday_to_g_time_ww
    1  = G_WEEKDAY_MON
    2  = G_WEEKDAY_TUE
    4  = G_WEEKDAY_WED
    8  = G_WEEKDAY_THU
    16 = G_WEEKDAY_FRI
    32 = G_WEEKDAY_SAT
    64 = G_WEEKDAY_SUN
endhash


const AM = 0
const PM = 1