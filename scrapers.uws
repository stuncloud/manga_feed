function get_scrapers()
    items = [
        Scraper(
            "スターウォーク",
            'https://webcomicgamma.takeshobo.co.jp/manga/starwalk/',
            ScrapeComicGamma,
            WeekDay.Fri, 12
        ),
        Scraper(
            "ウは宇宙ヤバイのウ！",
            'https://webcomicgamma.takeshobo.co.jp/manga/uchuuyabai/',
            ScrapeComicGamma,
            WeekDay.Fri, 12
        ),
        Scraper(
            "児玉まりあ文学集成",
            'https://to-ti.in/product/mariakodama',
            ScrapeToti,
            WeekDay.Sun, 0
        ),
        Scraper(
            "無能なナナ",
            'https://www.ganganonline.com/title/72',
            ScrapeGanGanOnline,
            WeekDay.Sat, 8
        ),
        Scraper(
            "ブレイド＆バスタード",
            'https://drecom-media.jp/drecomics/series/blade',
            ScrapeDrecom,
            WeekDay.Fri, 12
        ),
        Scraper(
            "鍋に弾丸を受けながら",
            'https://comic-walker.com/detail/KC_003969_S?episodeType=latest',
            ScrapeKadoComi,
            WeekDay.Fri, 0
        ),
        Scraper(
            "俺、勇者じゃないですから。",
            'https://bunshun.jp/list/author/60e7c5b077656106fe030000',
            ScrapeBunshun,
            WeekDay.Fri, 8
        ),
        Scraper(
            "神引きのモナーク（仮）",
            'https://www.pixiv.net/user/1504297/series/198227',
            ScrapePixiv,
            WeekDay.Mon, 4
        ),
        Scraper(
            "ニセモノの錬金術師",
            'https://www.pixiv.net/user/1504297/series/74316',
            ScrapePixiv,
            WeekDay.Tue, 4
        ),
        Scraper(
            "ニセモノの錬金術師：第二部",
            'https://www.pixiv.net/user/1504297/series/135359',
            ScrapePixiv,
            WeekDay.Wed, 4
        ),
        Scraper(
            "ニセモノの錬金術師：第百部",
            'https://www.pixiv.net/user/1504297/series/141994',
            ScrapePixiv,
            WeekDay.Thu, 4
        ),
        Scraper(
            "決闘手術",
            'https://www.pixiv.net/user/63740794/series/132782',
            ScrapePixiv,
            WeekDay.Sun, 4
        ),
    ]
    result = Scrapers(items)
fend

class Scrapers
    dim items
    procedure Scrapers(items: array)
        this.items = items
    fend
    function get(i: number)
        result = this.item[i]
    fend
    function get_by_title(title: string)
        result = EMPTY
        for item in items
            if pos(title, item.title()) > 0 then
                result = item
                break
            endif
        next
    fend
    function as_array()
        result = this.items
    fend
endclass

class PubDate
    dim dt_sec
    // constructor
    procedure PubDate(dt: number = 0)
        if dt > 0 then
            this.dt_sec = dt
        else
            this.dt_sec = gettime()
        endif
    fend

    function get_sec()
        result = this.dt_sec
    fend

    function to_string()
        dt_str = format(this.dt_sec, '%a, %d %h %Y %T %z')
        result = this.replace_day_month(dt_str)
    fend

    dim replace_day_month = function(dt_str: string)
        for month in [[' 1月', 'Jan'], [' 2月', 'Feb'], [' 3月', 'Mar'], [' 4月', 'Apr'], [' 5月', 'May'], [' 6月', 'Jun'], [' 7月', 'Jul'], [' 8月', 'Aug'], [' 9月', 'Sep'], ['10月', 'Oct'], ['11月', 'Nov'], ['12月', 'Dec']]
            if pos(month[0], dt_str) > 0 then
                dt_str = replace(dt_str, month[0], month[1])
                break
            endif
        next
        for day in [['日', 'Sun'], ['月', 'Mon'], ['火', 'Tue'], ['水', 'Wed'], ['木', 'Thu'], ['金', 'Fri'], ['土', 'Sat']]
            if pos(day[0], dt_str) > 0 then
                dt_str = replace(dt_str, day[0], day[1])
                break
            endif
        next
        result = dt_str
    fend
endclass

function get_feed_object(title, link, description, pub_dt_sec: number = 0)
    result = @{
        "title": null,
        "link": null,
        "pubDate": null,
        "pubSec": null,
        "description": null
    }@
    result.title = title
    result.link = link
    result.description = description
    pd = PubDate(pub_dt_sec)
    result.pubDate = pd.to_string()
    result.pubSec = pd.get_sec()
fend

function convert_date_to_sec(date, offset = 0, offset_opt = G_OFFSET_HOURS)
    select TRUE
        // 年月日 表記
        // コミックガンマ
        // DRECOM
        case length(m := Match(date, '(\d{4})年(\d{1,2})月(\d{1,2})日')) > 0
            yyyy = m[0][1]
            mm = format(val(m[0][2]), 2, 0, FMT_ZERO)
            dd = format(val(m[0][3]), 2, 0, FMT_ZERO)
            result = gettime(offset, "<#yyyy>/<#mm>/<#dd>", offset_opt)
        // yy/mm/dd 表記
        // トーチ
        case length(m := Match(date, "\d{2}/\d{2}/\d{2}")) > 0
            d = '20' + m[0]
            result = gettime(offset, d, offset_opt)
        // yyyy.mm.dd 表記
        // ガンガンONLINE
        case length(m := Match(date, "\d{4}\.\d{2}\.\d{2}")) > 0
            d = replace(m[0], '.', '/')
            result = gettime(offset, d, offset_opt)
        // yyyy/mm/dd 表記
        // カドコミ
        case length(m := Match(date, "\d{4}/\d{2}/\d{2}")) > 0
            d = m[0]
            result = gettime(offset, d, offset_opt)
        // yyyy-mm-dd hh:MM 表記
        // 文春オンライン
        case length(m := Match(date, "\d{4}-\d{2}-\d{2} \d{2}:\d{2}")) > 0
            d = m[0] + ':00'
            result = gettime(offset, d, offset_opt)
        default
            result = 0
    selend
fend

function ScrapePixiv(key: string, url: string, ignore_check: bool = FALSE, page = 1)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        result = []
        links = tab.document.querySelectorAll('main ul li a:nth-last-child(1)')
        should_go_on = false
        for link, i in links
            should_go_on = FeedScraperHelper.check(key, link.href)
            if ignore_check or should_go_on then
                print link.href
                print link.textContent
                subtitle = trim( link.textContent )
                subtitle = "<#subtitle> (<#key>)"
                summary = trim( subtitle )
                pub_sec = 0
                result += get_feed_object(subtitle, link.href, summary, pub_sec)
                if i == 0 then
                    FeedScraperHelper.update(key, link.href)
                endif
            else
                break
            endif
        next
        // 2ページ目まで見る
        if ! should_go_on and length(result) == length(links) and page < 2 then
            page += 1
            for a in tab.document.querySelectorAll('main nav a')
                if trim( a.textContent ) == "<#page>" then
                    a.click()
                    tab.wait()
                    more = ScrapePixiv(key, a.href, ignore_check, page)
                    for item in more
                        result += item
                    next
                    break
                endif
            next
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeBunshun(key: string, url: string, ignore_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        li = tab.document.querySelector('#page-content ul li')
        link = li.querySelector('.title a')
        if ignore_check or FeedScraperHelper.check(key, link.href) then
            subtitle = trim( li.querySelector('.subtitle').textContent )
            summary = trim( link.textContent )
            pub_sec = convert_date_to_sec(li.querySelector('time').dateTime, 0)
            result = get_feed_object(subtitle, link.href, summary, pub_sec)
            FeedScraperHelper.update(key, link.href)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeKadoComi(key: string, url: string, ignore_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        root = tab.document.querySelector('.EpisodesTabContents_root__VrJZ1')
        link = root.querySelector('ul li a')
        if ignore_check or FeedScraperHelper.check(key, link.href) then
            subtitle = trim( link.querySelector('.EpisodeThumbnail_title__G1eWj').textContent )
            subtitle = "<#subtitle> (<#key>)"
            summary = trim( root.querySelector('.EpisodesTabContents_nextUpdateDate__YDQiC').textContent )
            pub_sec = convert_date_to_sec(trim( link.querySelector('span').textContent ), 0)
            result = get_feed_object(subtitle, link.href, summary, pub_sec)
            FeedScraperHelper.update(key, link.href)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeDrecom(key: string, url: string, ignore_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        div = tab.document.querySelector('.ebookListItem')
        link = div.querySelector('a').href
        if ignore_check or FeedScraperHelper.check(key, link) then
            subtitle = trim(div.querySelector('.ebookListItem_title').textContent)
            subtitle = "<#subtitle> (<#key>)"
            spans = div.querySelectorAll('.ebookListItem_publishDate span')
            summary = trim( spans[1].textContent )
            pub_sec = convert_date_to_sec(trim( spans[0].textContent ), 12)
            result = get_feed_object(subtitle, link, summary, pub_sec)
            FeedScraperHelper.update(key, link)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeGanGanOnline(key: string, url: string, ignore_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        link = tab.document.querySelectorAll('.Chapter_chapter__oSTBF')[1]
        inner = link.querySelectorAll('.Chapter_chapter__body__Iopda p')
        episode = trim( inner[0].textContent )
        subtitle = "<#episode> (<#key>)"
        summary = trim( inner[1].textContent )
        if ignore_check or FeedScraperHelper.check(key, episode) then
            pub_sec = convert_date_to_sec(trim( inner[2].textContent ), 8)
            // urlを得るために実際にリンクをクリック
            link.click()
            // 読み込み待ち
            tab.wait()
            // URL取得
            sleep(1)
            href = tab.document.location.href
            result = get_feed_object(subtitle, href, summary, pub_sec)
            FeedScraperHelper.update(key, episode)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeComicGamma(key: string, url: string, ignore_check: bool = FALSE)
    result = EMPTY
    tab = FeedScraperHelper.browser.new(url)
    try
        link = tab.document.querySelector('div.read__outer a')
        if ignore_check or FeedScraperHelper.check(key, link.id) then
            subtitle = trim( link.querySelector('.episode').textContent )
            subtitle = "<#subtitle> (<#key>)"
            updated = link.querySelector('.episode__text').textContent
            pub_sec = convert_date_to_sec(updated, 12)
            summary = '公開日: ' + updated
            result = get_feed_object(subtitle, link.href, summary, pub_sec)
            FeedScraperHelper.update(key, link.id)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

function ScrapeToti(key: string, url: string, ignore_check: bool = FALSE)
    result = EMPTY

    tab = FeedScraperHelper.browser.new(url)
    try
        link = tab.document.querySelector('.next a')
        if ignore_check or FeedScraperHelper.check(key, link.href) then
            title = trim( link.querySelector('span.typesquare_option').textContent )
            summary = trim( tab.document.querySelector('header time').textContent )
            pub_sec = convert_date_to_sec(summary)
            result = get_feed_object(title, link.href, summary, pub_sec)
            FeedScraperHelper.update(key, link.href)
        endif
    except
        result = get_feed_object("[解析エラー] <#key>", url, "<#TRY_ERRLINE><#CR><#TRY_ERRMSG>")
    endtry
    tab.close()
fend

module FeedScraperHelper
    const HEADLESS = FALSE
    const MANGA_FEED_PROFILE_DIR = "<#GET_CUR_DIR>\profile\"
    // const ERROR_LOG = 'D:\manga_feed\error.log'
    public browser
    // constructor
    procedure FeedScraperHelper
        browser = browserbuilder(BC_MSEDGE)_
            .headless(HEADLESS)_
            .profile(MANGA_FEED_PROFILE_DIR)_
            .start()
    fend

    const ini = 'manga_feed.ini'
    const section = 'manga_feed'

    function check(key, value)
        recent = readini(section, key, ini)
        result = value != recent
    fend

    procedure update(key, value)
        writeini(section, key, value, ini)
    fend

    // procedure error(msg)
    //     fid = fopen(ERROR_LOG, F_WRITE8 or F_AUTOCLOSE)
    //     fput(fid, msg)
    // fend
endmodule


class Scraper
    // タイトル
    dim title
    // URL
    dim url
    // 曜日
    dim day
    // 時刻
    dim time
    // スクレイピング用関数
    dim fn
    // constructor
    procedure Scraper(title: string, url: string, fn: func, day: WeekDay, time: number)
        this.title = title
        this.url = url
        this.fn = fn
        this.day = day
        this.time = time
    fend

    // 指定時刻かどうか
    // dはG_TIME_WW
    function check(d, h)
        result = int(time) == int(h)
        if this.day == WeekDay.Any then
            exit
        endif
        d = convert_weekday[d]
        result = ((this.day andb d) == d) and result
    fend

    // スクレイピング用関数はUObject、またはEMPTYを返す
    function scrape(ignore_check = FALSE)
        result = fn(this.title, this.url, ignore_check)
    fend

    function title()
        result = this.title
    fend
endclass

enum WeekDay
    Mon = 1
    Tue = 2
    Wed = 4
    Thu = 8
    Fri = 16
    Sat = 32
    Sun = 64
    Any = 128
endenum
hash public convert_weekday
    "<#G_WEEKDAY_MON>" = WeekDay.Mon
    "<#G_WEEKDAY_TUE>" = WeekDay.Tue
    "<#G_WEEKDAY_WED>" = WeekDay.Wed
    "<#G_WEEKDAY_THU>" = WeekDay.Thu
    "<#G_WEEKDAY_FRI>" = WeekDay.Fri
    "<#G_WEEKDAY_SAT>" = WeekDay.Sat
    "<#G_WEEKDAY_SUN>" = WeekDay.Sun
endhash
